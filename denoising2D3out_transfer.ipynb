{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "denoising2D3out_transfer.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yskuchi/wf_denoising/blob/master/denoising2D3out_transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktKu9m1u5WtO"
      },
      "source": [
        "# Waveform denoising 'denoising2D3out_transfer'\n",
        "Author: Yusuke Uchiyama\n",
        "\n",
        "A denoising & deconvolution convolutional autoencoder with Tensorflow2.x\n",
        "applied to **a set of** waveform data.  \n",
        "Outputs are (0) estimated noise waveform, (1) estimated signal waveform, (2) deconvolved impulse signal (= cluster timing). \n",
        "See [Bitbucket repository](https://bitbucket.org/meg_ilc_tokyo/wf_denoising/src/master/) or \n",
        "[GitHub repository](https://github.com/yskuchi/wf_denoising)\n",
        "\n",
        "Training start with trained model for denoising part (transfer learning).\n",
        "\n",
        "Noise from data is added to MC signal data.\n",
        "You need datasets of signal and noise, separately, in pickle format.\n",
        "\n",
        "## Environment\n",
        "As of 2022 Aug, tested with the following:\n",
        "\n",
        "* Google Colab\n",
        "* CPU, GPU, or TPU (experimental)\n",
        "* Python 3.7\n",
        "* TensorFlow 2.8.2\n",
        "* Comet ML 3.31\n",
        "\n",
        "\n",
        "Note: If you are running this in a colab notebook, we recommend you enable a free GPU by going:\n",
        "> Runtime   →   Change runtime type   →   Hardware Accelerator: GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM0Fq4Mk53Uz"
      },
      "source": [
        "## Setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlAF_nP-58qi"
      },
      "source": [
        "### Comet ML"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA68Tj_B4pIf"
      },
      "source": [
        "! pip install typing-extensions comet-ml\n",
        "! pip install typing-extensions\n",
        "#! [ ! -z \"$COLAB_GPU\" ] && pip install typing-extensions==3.7.4 comet-ml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHB2psDB88Nb"
      },
      "source": [
        "# import comet_ml in the top of your file\n",
        "from comet_ml import Experiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88WRgbDT6DHb"
      },
      "source": [
        "### Other packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY8VDwiq5xlT"
      },
      "source": [
        "import os, sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import gc\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose\n",
        "from tensorflow.keras.layers import Dense, Reshape, Flatten, Concatenate, Add, Subtract\n",
        "from tensorflow.keras.layers import Dropout, Activation, BatchNormalization\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "print ('Python version: ' + str(sys.version_info))\n",
        "print ('TensorFlow version: ' + str(tf.__version__))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkXBecYwk9sE"
      },
      "source": [
        "### GPU\n",
        "To use GPU on Google colab, specify GPU in runtime type. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3HXKdnVjbYD"
      },
      "source": [
        "# check GPU\n",
        "tf.test.gpu_device_name()\n",
        "!echo $COLAB_GPU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bo4b4UfblDU2"
      },
      "source": [
        "### TPU\n",
        "To use TPU on Google colab, it is not enough to specify TPU in runtime type.\n",
        "See \"Setup TPU\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooBISQSClYZG"
      },
      "source": [
        "# check TPU\n",
        "!echo $COLAB_TPU_ADDR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR8khwIbBVZK"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKGwPpA4BgWi"
      },
      "source": [
        "# arg\n",
        "load_weights = False\n",
        "plot_data = True \n",
        "\n",
        "import matplotlib\n",
        "if not plot_data:\n",
        "    matplotlib.use(\"Agg\") # this is necessary when using plt without display (batch)\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3Y1ACnmB5PH"
      },
      "source": [
        "# infer signal or noise?\n",
        "extract_signal = False\n",
        "\n",
        "# Number of channels (if 2, two-end signals are dealt with channel)\n",
        "nchannels = 2\n",
        "\n",
        "# Number of waveforms packed in an input data (= Hight of 2D data)\n",
        "height = 8\n",
        "\n",
        "if extract_signal:\n",
        "  filename = f'denoising2D{height}3out_tf2'\n",
        "else:\n",
        "  filename = f'noiseextraction2D{height}ch3out_tf2'\n",
        "\n",
        "# Waveform has 1024 sample-points\n",
        "npoints = 512 #1024 # 256 # number of sample-points to be used (= Width of 2D data)\n",
        "scale = 1 #5\n",
        "offset = 0.000 #0.05 # 50 mV\n",
        "\n",
        "signal_scales = [1]  # multiple scales -> data augmentation\n",
        "\n",
        "fine_tuning = True  # flag to apply fine-tuning\n",
        "\n",
        "#signal_dataset_file = 'wf11600.pkl.gz'\n",
        "##noise_dataset_file  = 'wf328469.pkl' #2018\n",
        "#noise_dataset_file  = 'wf356990.pkl.gz' #2020\n",
        "signal_dataset_file = 'wf_cdch13000.pkl.gz'\n",
        "noise_dataset_file  = 'wf402042.pkl.gz' #2021\n",
        "cluster_dataset_file = 'cls1st_cdch13000.pkl.gz' \n",
        "\n",
        "# base model\n",
        "base_model_filename = 'noiseextraction2D8ch2out_tf2_20220824_2.h5'\n",
        "base_model_weights_filename = 'noiseextraction2D8ch2out_tf2_20220824_2_weights.h5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0zbN01DB-Tt"
      },
      "source": [
        "#### Hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ic5qtU4HCCWY"
      },
      "source": [
        "nkernels = [128, 128, 64,   64, 128, 128]\n",
        "\n",
        "kernel_size = [[1,7], [3,7], [3,5], [1,7], [1,7], [1,5]]\n",
        "strides =[[1,1], [1,1], [1,1]]\n",
        "pooling_size = [[1,2],[2,2],[2,2], [1,2],[1,2], [1,2]]\n",
        "\n",
        "# basic hyper-parameters\n",
        "params = {\n",
        "    'optimizer':   'adam',\n",
        "    'loss':        'mse', #'mae', #'msle', #'binary_crossentropy', \n",
        "    'loss_weights': [0.15, 0.05, 0.8],\n",
        "    'metrics':     ['mae', 'mse'],\n",
        "    'epochs':      1,#50, #150, # 20,\n",
        "    'batch_size':  1024, #512, #256,\n",
        "    'steps_per_execution': 100, # default 1, for TPU larger value may help\n",
        "}\n",
        "if extract_signal:\n",
        "  params['loss'] = 'msle'\n",
        "\n",
        "# additional parameters\n",
        "params2 = {\n",
        "    'loss_type':           params['loss'],\n",
        "    'conv_activation':     'relu',\n",
        "    'output_activation':   ['linear', 'sigmoid'],\n",
        "    'signal_dataset_file': signal_dataset_file,\n",
        "    'noise_dataset_file':  noise_dataset_file,\n",
        "    'File name':           filename + '.ipynb',\n",
        "    'nchannels':           nchannels,\n",
        "    'nrows':               height,\n",
        "    'npoints':             npoints,\n",
        "    'batch_size':          params['batch_size'],\n",
        "    'scale':               scale,\n",
        "    'offset':              offset,\n",
        "    'nkernels':            nkernels,\n",
        "    'kernel_size':         kernel_size,\n",
        "    'skip_connection':     [True, True, True, True, True, True], # skip connections for UNet-like structure\n",
        "    'nsublayers':          1,\n",
        "    'batch_norm':          [0,0,0, 0,1,0,0,1,0]\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krnakVkaFChi"
      },
      "source": [
        "## Prepare datasets\n",
        "On Google Colb, data is loaded via Google Drive.\n",
        "Files are supposed to be in `/content/drive/My Drive/ML/data`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcH3OZVHFa4M"
      },
      "source": [
        "### Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oxb-B8-_FSfK"
      },
      "source": [
        "## The following doesn't work anymore for Drive in other account (as of 2022 Aug).\n",
        "## To mount Drive in other account, do the three steps below\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#data_dir = '/content/drive/My Drive/ML/data/'\n",
        "#output_dir = '/content/drive/My Drive/ML/results/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Here is a trick to mount Drive in other account\n",
        "## First, run this column and click the URL shown in the error message.\n",
        "!sudo add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!sudo apt-get update -qq 2>&1 > /dev/null\n",
        "!sudo apt -y install -qq google-drive-ocamlfuse 2>&1 > /dev/null\n",
        "!google-drive-ocamlfuse"
      ],
      "metadata": {
        "id": "J55razGlv0o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install -qq w3m # to act as web browser \n",
        "!xdg-settings set default-web-browser w3m.desktop # to set default browser\n",
        "%cd /content\n",
        "!mkdir drive\n",
        "%cd drive\n",
        "!mkdir MyDrive\n",
        "%cd ..\n",
        "%cd ..\n",
        "!google-drive-ocamlfuse /content/drive/MyDrive"
      ],
      "metadata": {
        "id": "BAPJnnQ0v6cT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/MyDrive/ML/data/'\n",
        "output_dir = '/content/drive/MyDrive/ML/results/'"
      ],
      "metadata": {
        "id": "OFFb5Ikm3YHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUqm1p9IImg_"
      },
      "source": [
        "### Load pickle files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FgrjGTQIIbA"
      },
      "source": [
        "x_signal = pd.read_pickle(data_dir+signal_dataset_file).to_numpy()\n",
        "x_noise = pd.read_pickle(data_dir+noise_dataset_file ).to_numpy()\n",
        "x_cluster = pd.read_pickle(data_dir+cluster_dataset_file ).to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data-augmentation with different scales\n",
        "print(x_signal.shape)\n",
        "x_signal_ori = x_signal\n",
        "x_cluster_ori = x_cluster\n",
        "for signal_scale in signal_scales:\n",
        "  if signal_scale != 1.0:\n",
        "    x_scaled = x_signal_ori * signal_scale\n",
        "    x_signal = np.concatenate([x_signal, x_scaled])\n",
        "    x_cluster_scaled = x_cluster_ori * signal_scale\n",
        "    x_cluster = np.concatenate([x_cluster, x_cluster_scaled])\n",
        "print(x_signal.shape)\n",
        "del x_signal_ori\n",
        "del x_cluster_ori\n",
        "\n",
        "# For noise data, repeat if fewer than signal data\n",
        "x_noise_ori = x_noise\n",
        "while True:\n",
        "  if len(x_noise) > len(x_signal):\n",
        "    break\n",
        "  x_noise = np.concatenate([x_noise, x_noise_ori])\n",
        "del x_noise_ori\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "8mtZFq_g_cGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "derkAEziHewF"
      },
      "source": [
        "nsamples = min(len(x_signal), len(x_noise)) \n",
        "nsamples = int(nsamples / (nchannels * height)) * nchannels * height\n",
        "\n",
        "print(f'signal samples:{len(x_signal)}, noise samples:{len(x_noise)}, nsamples: {nsamples}')\n",
        "\n",
        "if extract_signal:\n",
        "  print('Extract signal')\n",
        "  x_tobe_removed = x_noise[0:nsamples]\n",
        "  x_tobe_extracted = x_signal[0:nsamples]\n",
        "else:\n",
        "  print('Extract noise')\n",
        "  x_tobe_removed = x_signal[0:nsamples]\n",
        "  x_tobe_extracted = x_noise[0:nsamples]\n",
        "x_tobe_compared = x_cluster[0:nsamples]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1hGjvK5JgIn"
      },
      "source": [
        "### Shape data in appropriate format with adding noise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFOl1111JivR"
      },
      "source": [
        "x_tobe_removed = x_tobe_removed.astype('float32')\n",
        "x_tobe_removed = x_tobe_removed.T[-npoints:].T # keep last npoints\n",
        "x_tobe_extracted = x_tobe_extracted.astype('float32')\n",
        "x_tobe_extracted = x_tobe_extracted.T[-npoints:].T # keep last npoints\n",
        "x_tobe_compared = x_tobe_compared.astype('float32')\n",
        "x_tobe_compared = x_tobe_compared.T[-npoints:].T # keep last npoints\n",
        "\n",
        "# Add noise\n",
        "x_train_noisy = x_tobe_removed + x_tobe_extracted\n",
        "\n",
        "# Adjust scale and offset of waveforms\n",
        "x_tobe_removed *= scale\n",
        "x_tobe_removed += offset * scale\n",
        "x_tobe_extracted *= scale # scale\n",
        "x_tobe_extracted += offset * scale;\n",
        "x_train_noisy *= scale # scale\n",
        "x_train_noisy += offset * scale; # add offset\n",
        "x_tobe_compared *= scale\n",
        "\n",
        "## Values in [0,1]\n",
        "#x_tobe_extracted = np.clip(x_tobe_extracted, 0, 1);\n",
        "#x_train_noisy = np.clip(x_train_noisy, 0, 1);\n",
        "\n",
        "# To match the input shape for Conv2D with n channels\n",
        "ninputs = int(len(x_tobe_removed) / (nchannels * height))\n",
        "x_tobe_removed = np.reshape(x_tobe_removed, (ninputs, height, nchannels, npoints)).transpose(0,1,3,2)\n",
        "x_tobe_extracted = np.reshape(x_tobe_extracted, (ninputs, height, nchannels, npoints)).transpose(0,1,3,2)\n",
        "x_train_noisy = np.reshape(x_train_noisy, (ninputs, height, nchannels, npoints)).transpose(0,1,3,2)\n",
        "x_tobe_compared = np.reshape(x_tobe_compared, (ninputs, height, nchannels, npoints)).transpose(0,1,3,2)\n",
        "\n",
        "print(x_tobe_removed.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0IXI_u4CKCN"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgTOvIWiGeJR"
      },
      "source": [
        "# Add the following code anywhere in your machine learning file\n",
        "# api_key and workspace are supposed to be set in .comet.config file,\n",
        "# otherwise set here like Experiment(api_key=\"AAAXXX\", workspace = \"yyy\", project_name=\"zzz\")\n",
        "# experiment = Experiment(project_name=\"wf_denoising\")\n",
        "experiment = Experiment(api_key=\"gBJn86Y1oAYKM2oxaoY0oV4Af\", workspace=\"yskuchi\", project_name=\"wf_denoising\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf7QQHDhTykM"
      },
      "source": [
        "experiment.log_parameters(params2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwS8TkGUlopw"
      },
      "source": [
        "### Setup TPU\n",
        "This part seems tf version dependent and may be changed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZK47WZDls95"
      },
      "source": [
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  tpu_grpc_url = \"grpc://\"+os.environ[\"COLAB_TPU_ADDR\"]\n",
        "  tpu_cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu_grpc_url)\n",
        "  tf.config.experimental_connect_to_cluster(tpu_cluster_resolver) # TF2.0の場合、ここを追加\n",
        "  tf.tpu.experimental.initialize_tpu_system(tpu_cluster_resolver) # TF2.0の場合、今後experimentialが取れる可能性がある    \n",
        "  #strategy = tf.distribute.experimental.TPUStrategy(tpu_cluster_resolver)  # ここも同様\n",
        "  strategy = tf.distribute.TPUStrategy(tpu_cluster_resolver)  # experimentalいらなくなった\n",
        "  #model = tf.distribute.tpu.keras_to_tpu_model(model, strategy=strategy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iLEhDoLzeYQ"
      },
      "source": [
        "### Build model with functional API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yusCHYE3CNTD"
      },
      "source": [
        "def build_model():\n",
        "\n",
        "  # denoising part\n",
        "  base_model = load_model(output_dir + base_model_filename)\n",
        "  base_model.summary()\n",
        "  input_img = base_model.input\n",
        "  decoded1 = base_model.get_layer('decoded1').output\n",
        "  decoded2 = base_model.get_layer('decoded2').output\n",
        "\n",
        "  # deconvolution part\n",
        "  conv1 = Conv2D(params2['nkernels'][0], (kernel_size[3][0], kernel_size[3][1]), strides=(strides[0][0], strides[0][1]), padding='same', name='conv1_deco')(decoded2)\n",
        "  if params2['batch_norm'][3]:\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "  #conv1 = Dropout(0.2)(conv1)\n",
        "  conv1 = Activation(params2['conv_activation'])(conv1)\n",
        "  for i in range(1, params2['nsublayers']):\n",
        "    conv1 = Conv2D(params2['nkernels'][0], (kernel_size[3][0], kernel_size[3][1]), strides=(1,1), padding='same', activation=params2['conv_activation'], name=f'conv1_{i}_deco')(conv1)\n",
        "  pool1 = MaxPooling2D((pooling_size[3][0], pooling_size[3][1]), padding='same', name='max_pooling2d_1_deco')(conv1)\n",
        "  conv2 = Conv2D(params2['nkernels'][1], (kernel_size[4][0], kernel_size[4][1]), strides=(strides[1][0], strides[1][1]), padding='same', name='conv2_deco')(pool1)\n",
        "  if params2['batch_norm'][4]:\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "  #conv2 = Dropout(0.2)(conv2)\n",
        "  conv2 = Activation(params2['conv_activation'])(conv2)\n",
        "  for i in range(1, params2['nsublayers']):\n",
        "    conv2 = Conv2D(params2['nkernels'][1], (kernel_size[4][0], kernel_size[4][1]), strides=(1,1), padding='same', activation=params2['conv_activation'], name=f'conv2_{i}_deco')(conv2)\n",
        "  pool2 = MaxPooling2D((pooling_size[4][0], pooling_size[4][1]), padding='same', name='max_pooling2d_2_deco')(conv2)\n",
        "  conv3 = Conv2D(params2['nkernels'][2], (kernel_size[5][0], kernel_size[5][1]), strides=(strides[2][0], strides[2][1]), padding='same', name='conv3_deco')(pool2)\n",
        "  if params2['batch_norm'][5]:\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "  #conv3 = Dropout(0.2)(conv3)\n",
        "  conv3 = Activation(params2['conv_activation'])(conv3)\n",
        "  for i in range(1, params2['nsublayers']):\n",
        "    conv3 = Conv2D(params2['nkernels'][2], (kernel_size[5][0], kernel_size[5][1]), strides=(1,1), padding='same', activation=params2['conv_activation'], name=f'conv3_{i}_deco')(conv3)\n",
        "  encoded = MaxPooling2D((pooling_size[5][0], pooling_size[5][1]), padding='same')(conv3)\n",
        "\n",
        "  conv4 = Conv2D(params2['nkernels'][3], (kernel_size[5][0], kernel_size[5][1]), strides=(1, 1), padding='same', name='conv4_deco')(encoded)\n",
        "  #conv4 = Conv2DTranspose(params2['nkernels'][3], (kernel_size[5][0], kernel_size[5][1]), strides=(1, 1), padding='same')(encoded)\n",
        "  if params2['batch_norm'][6]:\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "  #conv4 = Dropout(0.2)(conv4)\n",
        "  conv4 = Activation(params2['conv_activation'])(conv4)\n",
        "  for i in range(1, params2['nsublayers']):\n",
        "    conv4 = Conv2D(params2['nkernels'][3], (kernel_size[5][0], kernel_size[5][1]), strides=(1,1), padding='same', activation=params2['conv_activation'], name=f'conv4_{i}_deco')(conv4)\n",
        "  conv4 = UpSampling2D((pooling_size[5][0], pooling_size[5][1]), name='up_sampling2d_1_deco')(conv4)\n",
        "  if params2['skip_connection'][5]:\n",
        "    conv4 = Concatenate(name='concat1_deco')([conv4, conv3])\n",
        "  conv5 = Conv2DTranspose(params2['nkernels'][4], (kernel_size[4][0], kernel_size[4][1]), strides=(strides[2][0], strides[2][1]), padding='same', name='convT5_deco')(conv4)\n",
        "  if params2['batch_norm'][7]:\n",
        "    conv5 = BatchNormalization()(conv5)\n",
        "  #conv5 = Dropout(0.2)(conv5)\n",
        "  conv5 = Activation(params2['conv_activation'])(conv5)\n",
        "  for i in range(1, params2['nsublayers']):\n",
        "    conv5 = Conv2D(params2['nkernels'][4], (kernel_size[4][0], kernel_size[4][1]), strides=(1,1), padding='same', activation=params2['conv_activation'], name=f'convT5_{i}_deco')(conv5)\n",
        "  conv5 = UpSampling2D((pooling_size[4][0], pooling_size[4][1]), name='up_sampling2d_2_deco')(conv5)\n",
        "  if params2['skip_connection'][4]:\n",
        "    conv5 = Concatenate(name='concat2_deco')([conv5, conv2])\n",
        "  conv6 = Conv2DTranspose(params2['nkernels'][5], (kernel_size[3][0], kernel_size[3][1]), strides=(strides[1][0], strides[1][1]), padding='same', name='convT6_deco')(conv5)\n",
        "  if params2['batch_norm'][8]:\n",
        "    conv6 = BatchNormalization()(conv6)\n",
        "  #conv6 = Dropout(0.2)(conv6)\n",
        "  conv6 = Activation(params2['conv_activation'])(conv6)\n",
        "  for i in range(1, params2['nsublayers']):\n",
        "    conv6 = Conv2D(params2['nkernels'][5], (kernel_size[3][0], kernel_size[3][1]), strides=(1,1), padding='same', activation=params2['conv_activation'], name=f'convT6_{i}_deco')(conv6)\n",
        "  conv6 = UpSampling2D((pooling_size[3][0], pooling_size[3][1]), name='up_sampling2d_3_deco')(conv6)\n",
        "  if params2['skip_connection'][3]:\n",
        "    conv6 = Concatenate(name='concat3_deco')([conv6, conv1])\n",
        "  for i in range(params2['nsublayers'] - 1):\n",
        "    conv6 = Conv2D(nchannels, (1, 3), padding='same', activation=params2['conv_activation'], name='conv6_deco')(conv6)\n",
        "  decoded3 = Conv2DTranspose(nchannels, (1, 3), strides=(strides[0][0], strides[0][1]), padding='same', activation=params2['output_activation'][1], name='decoded3')(conv6)\n",
        "  \n",
        "\n",
        "  autoencoder = Model(inputs=input_img, outputs=[decoded1, decoded2, decoded3])\n",
        "\n",
        "  for layer in base_model.layers[:-1]: # 18 to fix all layers\n",
        "    layer.trainable = False\n",
        "\n",
        "  if params['steps_per_execution'] == 1:\n",
        "    autoencoder.compile(optimizer=params['optimizer'], \n",
        "                        loss={'decoded1': params['loss'], 'decoded2': 'msle', 'decoded3': 'binary_crossentropy'},\n",
        "                        loss_weights={'decoded1': params['loss_weights'][0], 'decoded2': params['loss_weights'][1], 'decoded3': params['loss_weights'][2]}, \n",
        "                        metrics=params['metrics'])\n",
        "  else:\n",
        "    autoencoder.compile(optimizer=params['optimizer'], \n",
        "                        loss={'decoded1': params['loss'], 'decoded2': 'msle', 'decoded3': 'binary_crossentropy'},\n",
        "                        loss_weights={'decoded1': params['loss_weights'][0], 'decoded2': params['loss_weights'][1], 'decoded3': params['loss_weights'][2]}, \n",
        "                        metrics=params['metrics'],\n",
        "                        steps_per_execution=params['steps_per_execution'])\n",
        "\n",
        "  autoencoder.summary()\n",
        "\n",
        "  #autoencoder.load_weights(f'{output_dir}{base_model_weights_filename}', by_name=True)\n",
        "  \n",
        "  return autoencoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldVS0BzqR9C9"
      },
      "source": [
        "try:\n",
        "  strategy\n",
        "  with strategy.scope():\n",
        "    autoencoder = build_model()\n",
        "except NameError:\n",
        "  autoencoder = build_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvNS_i04KgZC"
      },
      "source": [
        "## Fit\n",
        "\n",
        "On Google Colb, the results (trained model) are saved in Google Drive. Files are supposed to be in /content/drive/My Drive/ML/results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-o1mGrzJR6a"
      },
      "source": [
        "history=[]\n",
        "if not load_weights:\n",
        "\n",
        "    # Callback for model checkpoints\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        filepath = output_dir + filename + \"-{epoch:02d}.h5\",\n",
        "        save_best_only=True,\n",
        "        save_weight_only=False)\n",
        "    \n",
        "    # 'labels' are the pictures themselves\n",
        "    hist = autoencoder.fit(x_train_noisy,\n",
        "                           {'decoded1': x_tobe_extracted, 'decoded2': x_tobe_removed, 'decoded3': x_tobe_compared},\n",
        "                           epochs=params['epochs'],\n",
        "                           batch_size=params['batch_size'],\n",
        "                           shuffle=True,\n",
        "                           validation_split=0.1)#,\n",
        "                           #callbacks=[checkpoint])\n",
        "\n",
        "\n",
        "    # Save history\n",
        "    with open(output_dir + filename + '_hist.json', 'w') as f:\n",
        "        json.dump(hist.history, f)\n",
        "    history = hist.history\n",
        "        \n",
        "    # Save the weights\n",
        "    autoencoder.save_weights(output_dir + filename + '_weights.h5')\n",
        "else:\n",
        "    # Load weights\n",
        "    autoencoder.load_weights(f'{output_dir}{filename}_weights.h5')\n",
        "\n",
        "    # Load history\n",
        "    with open(f'{output_dir}{filename}_hist.json', 'r') as f:\n",
        "        history = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the model"
      ],
      "metadata": {
        "id": "nU1aFtWodAEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# as a keras model (a single .h5 file)\n",
        "autoencoder.save(output_dir + filename + '.h5', include_optimizer=False)\n",
        "# as a TF model (a directory) <- doesn't work with TPU ???\n",
        "# autoencoder.save(output_dir + filename)\n",
        "\n",
        "## Convert to ONNX\n",
        "#! pip install onnxmltools \n",
        "#! pip install tf2onnx\n",
        "#import onnxmltools\n",
        "#print ('ONNXMLTools version:' + str(onnxmltools.__version__))\n",
        "#onnx_model = onnxmltools.convert_keras(autoencoder)\n",
        "## Save as protobuf <- doesn't work with TPU ???\n",
        "#onnxmltools.utils.save_model(onnx_model, output_dir + filename + '.onnx')\n",
        "\n",
        "# Plot training history \n",
        "#plt.plot(history['loss'], linewidth=3, label='train')\n",
        "#plt.plot(history['val_loss'], linewidth=3, label='valid')\n",
        "plt.plot(history['decoded1_mae'], linewidth=3, label='train')\n",
        "plt.plot(history['val_decoded1_mae'], linewidth=3, label='valid')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "if params['metrics'] == 'mse':\n",
        "  plt.ylim(1e-6 * scale, 0.1e-3 * scale) #mse\n",
        "else:\n",
        "  plt.ylim(0.5e-3 * scale, 0.5e-2 * scale) #mae\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R8-gCJUkc-fU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine tuning\n",
        "Repeat training for a few layers "
      ],
      "metadata": {
        "id": "icVvFVo6yp4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if fine_tuning:\n",
        "  for layer in autoencoder.layers[:-1]: \n",
        "    layer.trainable = False\n",
        "  for layer in autoencoder.layers[13:18]:\n",
        "    layer.trainable = True\n",
        "  if params['steps_per_execution'] == 1:\n",
        "    autoencoder.compile(optimizer=RMSprop(learning_rate=1e-5), \n",
        "                        loss={'decoded1': params['loss'], 'decoded2': 'msle', 'decoded3': 'binary_crossentropy'},\n",
        "                        loss_weights={'decoded1': params['loss_weights'][0], 'decoded2': params['loss_weights'][1], 'decoded3': params['loss_weights'][2]}, \n",
        "                        metrics=params['metrics'])\n",
        "  else:\n",
        "    autoencoder.compile(optimizer=RMSprop(learning_rate=1e-5), \n",
        "                        loss={'decoded1': params['loss'], 'decoded2': 'msle', 'decoded3': 'binary_crossentropy'},\n",
        "                        loss_weights={'decoded1': params['loss_weights'][0], 'decoded2': params['loss_weights'][1], 'decoded3': params['loss_weights'][2]}, \n",
        "                        metrics=params['metrics'],\n",
        "                        steps_per_execution=params['steps_per_execution'])\n",
        "\n",
        "  autoencoder.summary()"
      ],
      "metadata": {
        "id": "Cse4SVJCy1mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZP1xolZ0GhI"
      },
      "source": [
        "if fine_tuning:\n",
        "  history=[]\n",
        "  hist = autoencoder.fit(x_train_noisy,\n",
        "                         {'decoded1': x_tobe_extracted, 'decoded2': x_tobe_removed, 'decoded3': x_tobe_compared},\n",
        "                          epochs=params['epochs'],\n",
        "                          batch_size=params['batch_size'],\n",
        "                          shuffle=True,\n",
        "                          validation_split=0.1)\n",
        "\n",
        "  # Save history\n",
        "  with open(output_dir + filename + '_hist.json', 'w') as f:\n",
        "      json.dump(hist.history, f)\n",
        "  history = hist.history\n",
        "        \n",
        "  # Save the weights\n",
        "  autoencoder.save_weights(output_dir + filename + '_weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if fine_tuning:\n",
        "  # as a keras model (a single .h5 file)\n",
        "  autoencoder.save(output_dir + filename + '.h5', include_optimizer=False)\n",
        "  # as a TF model (a directory) <- doesn't work with TPU ???\n",
        "  # autoencoder.save(output_dir + filename)\n",
        "\n",
        "  ## Convert to ONNX\n",
        "  #! pip install onnxmltools \n",
        "  #! pip install tf2onnx\n",
        "  #import onnxmltools\n",
        "  #print ('ONNXMLTools version:' + str(onnxmltools.__version__))\n",
        "  #onnx_model = onnxmltools.convert_keras(autoencoder)\n",
        "  ## Save as protobuf <- doesn't work with TPU ???\n",
        "  #onnxmltools.utils.save_model(onnx_model, output_dir + filename + '.onnx')\n",
        "\n",
        "  # Plot training history \n",
        "  #plt.plot(history['loss'], linewidth=3, label='train')\n",
        "  #plt.plot(history['val_loss'], linewidth=3, label='valid')\n",
        "  plt.plot(history['decoded1_mae'], linewidth=3, label='train')\n",
        "  plt.plot(history['val_decoded1_mae'], linewidth=3, label='valid')\n",
        "  plt.grid()\n",
        "  plt.legend()\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel('loss')\n",
        "  if params['metrics'] == 'mse':\n",
        "    plt.ylim(1e-6 * scale, 0.1e-3 * scale) #mse\n",
        "  else:\n",
        "    plt.ylim(0.5e-3 * scale, 0.5e-2 * scale) #mae\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "SDpTse0B0fEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ur-JWTe4WI0y"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fYGvPDyWLQz"
      },
      "source": [
        "x_test_tobe_removed = x_tobe_removed[0:]\n",
        "x_test_tobe_extracted = x_tobe_extracted[0:]\n",
        "x_test_noisy = x_train_noisy[0:]\n",
        "x_test_tobe_compared = x_tobe_compared[0:]\n",
        "print(x_test_noisy.shape)\n",
        "\n",
        "decoded_imgs, decoded_imgs2, decoded_imgs3 = autoencoder.predict(x_test_noisy)\n",
        "decoded_imgs = decoded_imgs[0:len(x_test_noisy)]\n",
        "decoded_imgs3 = decoded_imgs3[0:len(x_test_noisy)]\n",
        "print(decoded_imgs.shape)\n",
        "\n",
        "# revert scale and offset\n",
        "x_test_tobe_removed -= scale * offset\n",
        "x_test_tobe_removed /= scale\n",
        "x_test_tobe_extracted -= scale * offset\n",
        "x_test_tobe_extracted /= scale\n",
        "x_test_noisy -= scale * offset\n",
        "x_test_noisy /= scale\n",
        "x_test_tobe_compared /= scale\n",
        "decoded_imgs -= scale * offset\n",
        "decoded_imgs /= scale\n",
        "decoded_imgs3 /= scale\n",
        "\n",
        "print(x_test_tobe_extracted.shape)\n",
        "print(x_test_noisy.shape)\n",
        "\n",
        "x_test_tobe_removed = x_test_tobe_removed.transpose(0, 1, 3, 2)\n",
        "x_test_tobe_removed = np.reshape(x_test_tobe_removed, (len(x_test_tobe_removed) * height * nchannels, npoints))\n",
        "x_test_tobe_extracted = x_test_tobe_extracted.transpose(0, 1, 3, 2)\n",
        "x_test_tobe_extracted = np.reshape(x_test_tobe_extracted, (len(x_test_tobe_extracted) * height * nchannels, npoints))\n",
        "x_test_noisy = x_test_noisy.transpose(0,1,3,2)\n",
        "x_test_noisy = np.reshape(x_test_noisy, (len(x_test_noisy) * height * nchannels, npoints))\n",
        "x_test_tobe_compared = x_test_tobe_compared.transpose(0, 1, 3, 2)\n",
        "x_test_tobe_compared = np.reshape(x_test_tobe_compared, (len(x_test_tobe_compared) * height * nchannels, npoints))\n",
        "decoded_imgs = decoded_imgs.transpose(0,1,3,2)\n",
        "decoded_imgs = np.reshape(decoded_imgs, (len(decoded_imgs) * height * nchannels, npoints))\n",
        "decoded_imgs3 = decoded_imgs3.transpose(0,1,3,2)\n",
        "decoded_imgs3 = np.reshape(decoded_imgs3, (len(decoded_imgs3) * height * nchannels, npoints))\n",
        "\n",
        "x_residual = x_test_tobe_extracted - decoded_imgs\n",
        "x_decoded_signal = decoded_imgs\n",
        "if not extract_signal:\n",
        "  x_decoded_signal = x_test_noisy - decoded_imgs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYNfJ9rK0ouw"
      },
      "source": [
        "# How many waveforms to be displayed\n",
        "n = 2\n",
        "start = 1760\n",
        "fig = plt.figure(figsize=(20, 6 * n))\n",
        "j = 0\n",
        "for i in range(start, start + n):\n",
        "  ax = fig.add_subplot(n * 2, 1, j+1)\n",
        "  ax.plot(x_test_noisy[i], label=\"noisy\", color='gray')\n",
        "  if extract_signal:\n",
        "    ax.plot(x_test_tobe_extracted[i], label=\"signal\", color='green')\n",
        "  else:\n",
        "    ax.plot(x_test_tobe_extracted[i], label=\"noise\", color='green')\n",
        "    ax.plot(x_test_tobe_removed[i], label=\"signal\")\n",
        "    ax.plot(x_decoded_signal[i], label=\"extracted signal\")\n",
        "  ax.plot(decoded_imgs[i], label=\"decoded\", color='magenta')\n",
        "  ax.plot(x_residual[i], label=\"residual\")\n",
        "  ax.legend()\n",
        "  j += 1\n",
        "\n",
        "  ax = fig.add_subplot(n * 2, 1, j+1)\n",
        "  ax.plot(x_test_tobe_compared[i], label=\"signal\")\n",
        "  ax.plot(decoded_imgs3[i], label=\"decoded\", color='magenta')\n",
        "  ax.legend()\n",
        "  j += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H42-gn6xUocT"
      },
      "source": [
        "# Send this plot to comet\n",
        "experiment.log_figure(figure=fig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-jWr9AZJwXH"
      },
      "source": [
        "experiment.end()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXYB7Ai4VYhB"
      },
      "source": [
        "if plot_data:\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}