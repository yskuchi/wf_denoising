{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "noiseextraction2ch_tf2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOLviBw2UYrH4IiP1xBgdyl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yskuchi/wf_denoising/blob/master/noiseextraction2ch_tf2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktKu9m1u5WtO"
      },
      "source": [
        "# Waveform denoising 'noiseextraction2ch_tf2'\n",
        "Author: Yusuke Uchiyama\n",
        "\n",
        "A denoising convolutional autoencoder with Tensorflow2.x\n",
        "applied to **a set of** waveform data.  \n",
        "See [Bitbucket repository](https://bitbucket.org/meg_ilc_tokyo/wf_denoising/src/master/) or \n",
        "[GitHub repository](https://github.com/yskuchi/wf_denoising)\n",
        "\n",
        "Noise from data is added to MC signal data.\n",
        "You need datasets of signal and noise, separately, in pickle format.\n",
        "\n",
        "## Environment\n",
        "As of 2020 Nov, tested with the following:\n",
        "\n",
        "* Google Colab\n",
        "* CPU, GPU, or TPU (experimental)\n",
        "* Python 3.6\n",
        "* TensorFlow 2.3.0\n",
        "* Comet ML\n",
        "\n",
        "\n",
        "Note: If you are running this in a colab notebook, we recommend you enable a free GPU by going:\n",
        "> Runtime   →   Change runtime type   →   Hardware Accelerator: GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM0Fq4Mk53Uz"
      },
      "source": [
        "## Setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlAF_nP-58qi"
      },
      "source": [
        "### Comet ML"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfss5GotQBoh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA68Tj_B4pIf"
      },
      "source": [
        "! pip install typing-extensions==3.7.4 comet-ml\n",
        "! pip install typing-extensions==3.7.4\n",
        "#! [ ! -z \"$COLAB_GPU\" ] && pip install typing-extensions==3.7.4 comet-ml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHB2psDB88Nb"
      },
      "source": [
        "# import comet_ml in the top of your file\n",
        "from comet_ml import Experiment\n",
        "\n",
        "# Add the following code anywhere in your machine learning file\n",
        "# api_key and workspace are supposed to be set in .comet.config file,\n",
        "# otherwise set here like Experiment(api_key=\"AAAXXX\", workspace = \"yyy\", project_name=\"zzz\")\n",
        "# experiment = Experiment(project_name=\"wf_denoising\")\n",
        "experiment = Experiment(api_key=\"gBJn86Y1oAYKM2oxaoY0oV4Af\", workspace=\"yskuchi\", project_name=\"wf_denoising\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88WRgbDT6DHb"
      },
      "source": [
        "### Other packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY8VDwiq5xlT"
      },
      "source": [
        "import os, sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Conv1D, MaxPooling1D, UpSampling1D\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "print ('Python version: ' + str(sys.version_info))\n",
        "print ('TensorFlow version: ' + str(tf.__version__))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkXBecYwk9sE"
      },
      "source": [
        "### GPU\n",
        "To use GPU on Google colab, specify GPU in runtime type. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3HXKdnVjbYD"
      },
      "source": [
        "# check GPU\n",
        "tf.test.gpu_device_name()\n",
        "!echo $COLAB_GPU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bo4b4UfblDU2"
      },
      "source": [
        "### TPU\n",
        "To use TPU on Google colab, it is not enough to specify TPU in runtime type.\n",
        "See \"Setup TPU\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooBISQSClYZG"
      },
      "source": [
        "# check TPU\n",
        "!echo $COLAB_TPU_ADDR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR8khwIbBVZK"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKGwPpA4BgWi"
      },
      "source": [
        "# arg\n",
        "load_weights = False\n",
        "plot_data = True \n",
        "filename = \"noiseextraction5_tf2\"\n",
        "\n",
        "import matplotlib\n",
        "if not plot_data:\n",
        "    matplotlib.use(\"Agg\") # this is necessary when using plt without display (batch)\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3Y1ACnmB5PH"
      },
      "source": [
        "# Waveform has 1024 sample-points\n",
        "npoints = 1024 # 256 # number of sample-points to be used\n",
        "scale = 5\n",
        "offset = 0.05 # 50 mV\n",
        "\n",
        "# Number of channels (CNN channel = waveform channanels)\n",
        "nchannels = 2\n",
        "\n",
        "signal_dataset_file = 'wf11100.pkl'\n",
        "#noise_dataset_file  = 'wf328469.pkl' #2018\n",
        "noise_dataset_file  = 'wf356990.pkl' #2020"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0zbN01DB-Tt"
      },
      "source": [
        "#### Hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ic5qtU4HCCWY"
      },
      "source": [
        "# basic hyper-parameters\n",
        "params = {\n",
        "    'optimizer':   'adam',\n",
        "    'loss':        'mse', #'binary_crossentropy', \n",
        "    'epochs':      20, # 20,\n",
        "    'batch_size':  512, #256,\n",
        "}\n",
        "# additional parameters\n",
        "params2 = {\n",
        "    'conv_activation':     'relu',\n",
        "    'output_activation':   'linear', #'sigmoid',\n",
        "    'signal_dataset_file': signal_dataset_file,\n",
        "    'noise_dataset_file':  noise_dataset_file,\n",
        "    'npoints':             npoints,\n",
        "    'scale':               scale,\n",
        "    'offset':              offset,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf7QQHDhTykM"
      },
      "source": [
        "experiment.log_parameters(params2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krnakVkaFChi"
      },
      "source": [
        "## Prepare datasets\n",
        "On Google Colb, data is loaded via Google Drive.\n",
        "Files are supposed to be in `/content/drive/My Drive/ML/data`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcH3OZVHFa4M"
      },
      "source": [
        "### Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oxb-B8-_FSfK"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_dir = '/content/drive/My Drive/ML/data/'\n",
        "output_dir = '/content/drive/My Drive/ML/results/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUqm1p9IImg_"
      },
      "source": [
        "### Load pickle files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FgrjGTQIIbA"
      },
      "source": [
        "x_original = pd.read_pickle(data_dir+signal_dataset_file).to_numpy()\n",
        "x_noise = pd.read_pickle(data_dir+noise_dataset_file ).to_numpy()\n",
        "\n",
        "nsamples = min(len(x_original), len(x_noise)) \n",
        "nsamples = int(nsamples / nchannels) * nchannels\n",
        "\n",
        "print(f'signal samples:{len(x_original)}, noise samples:{len(x_noise)}, nsamples: {nsamples}')\n",
        "\n",
        "x_original = x_original[0:nsamples]\n",
        "x_noise = x_noise[0:nsamples]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1hGjvK5JgIn"
      },
      "source": [
        "### Shape data in appropriate format with adding noise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFOl1111JivR"
      },
      "source": [
        "x_original = x_original.astype('float32')\n",
        "x_original = x_original.T[-npoints:].T # keep last npoints\n",
        "x_noise = x_noise.astype('float32')\n",
        "x_noise = x_noise.T[-npoints:].T # keep last npoints\n",
        "\n",
        "# Add noise\n",
        "x_train_noisy = x_original + x_noise\n",
        "\n",
        "# Adjust scale and offset of waveforms\n",
        "x_noise *= scale # scale\n",
        "x_noise += offset * scale;\n",
        "x_train_noisy *= scale # scale\n",
        "x_train_noisy += offset * scale; # add 50 mV offset\n",
        "\n",
        "# Values in [0,1]\n",
        "x_noise = np.clip(x_noise, 0, 1);\n",
        "x_train_noisy = np.clip(x_train_noisy, 0, 1);\n",
        "\n",
        "# To match the input shape for Conv1D with 2 channel\n",
        "#x_noise = np.reshape(x_noise, (int(len(x_noise) / nchannels), npoints, nchannels))\n",
        "#x_train_noisy = np.reshape(x_train_noisy, (int(len(x_train_noisy) / nchannels), npoints, nchannels))\n",
        "x_original = np.reshape(x_original, (int(len(x_original) / nchannels), nchannels, npoints)).transpose(0,2,1)\n",
        "x_noise = np.reshape(x_noise, (int(len(x_noise) / nchannels), nchannels, npoints)).transpose(0,2,1)\n",
        "x_train_noisy = np.reshape(x_train_noisy, (int(len(x_train_noisy) / nchannels), nchannels, npoints)).transpose(0,2,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0IXI_u4CKCN"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwS8TkGUlopw"
      },
      "source": [
        "### Setup TPU\n",
        "This part seems tf version dependent and may be changed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZK47WZDls95"
      },
      "source": [
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  tpu_grpc_url = \"grpc://\"+os.environ[\"COLAB_TPU_ADDR\"]\n",
        "  tpu_cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu_grpc_url)\n",
        "  tf.config.experimental_connect_to_cluster(tpu_cluster_resolver) # TF2.0の場合、ここを追加\n",
        "  tf.tpu.experimental.initialize_tpu_system(tpu_cluster_resolver) # TF2.0の場合、今後experimentialが取れる可能性がある    \n",
        "  strategy = tf.distribute.experimental.TPUStrategy(tpu_cluster_resolver)  # ここも同様\n",
        "  #model = tf.distribute.tpu.keras_to_tpu_model(model, strategy=strategy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iLEhDoLzeYQ"
      },
      "source": [
        "### Build model with functional API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yusCHYE3CNTD"
      },
      "source": [
        "def build_model():\n",
        "  input_img = Input(shape=(npoints,nchannels))\n",
        "  x = Conv1D(64, 5, padding='same', activation=params2['conv_activation'])(input_img)\n",
        "  x = MaxPooling1D(2, padding='same')(x)\n",
        "  x = Conv1D(32, 5, padding='same', activation=params2['conv_activation'])(x)\n",
        "  x = MaxPooling1D(2, padding='same')(x)\n",
        "  x = Conv1D(32, 5, padding='same', activation=params2['conv_activation'])(x)\n",
        "  encoded = MaxPooling1D(2, padding='same')(x)\n",
        "\n",
        "  x = Conv1D(32, 5, padding='same', activation=params2['conv_activation'])(encoded)\n",
        "  x = UpSampling1D(2)(x)\n",
        "  x = Conv1D(32, 5, padding='same', activation=params2['conv_activation'])(x)\n",
        "  x = UpSampling1D(2)(x)\n",
        "  x = Conv1D(64, 5, padding='same', activation=params2['conv_activation'])(x)\n",
        "  x = UpSampling1D(2)(x)\n",
        "  decoded = Conv1D(nchannels, 5, padding='same', activation=params2['output_activation'])(x)\n",
        "\n",
        "  autoencoder = Model(inputs=input_img, outputs=decoded)\n",
        "\n",
        "  autoencoder.compile(optimizer=params['optimizer'], loss=params['loss']) \n",
        "  autoencoder.summary()\n",
        "  return autoencoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldVS0BzqR9C9"
      },
      "source": [
        "try:\n",
        "  strategy\n",
        "  with strategy.scope():\n",
        "    autoencoder = build_model()\n",
        "except NameError:\n",
        "  autoencoder = build_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvNS_i04KgZC"
      },
      "source": [
        "## Fit\n",
        "\n",
        "On Google Colb, the results (trained model) are saved in Google Drive. Files are supposed to be in /content/drive/My Drive/ML/results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-o1mGrzJR6a"
      },
      "source": [
        "history=[]\n",
        "if not load_weights:\n",
        "\n",
        "    # Callback for model checkpoints\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        filepath = output_dir + filename + \"-{epoch:02d}.h5\",\n",
        "        save_best_only=True,\n",
        "        save_weight_only=False)\n",
        "    \n",
        "    # 'labels' are the pictures themselves\n",
        "    hist = autoencoder.fit(x_train_noisy, x_noise,\n",
        "                           epochs=params['epochs'],\n",
        "                           batch_size=params['batch_size'],\n",
        "                           shuffle=True,\n",
        "                           validation_split=0.1,\n",
        "                           callbacks=[checkpoint])\n",
        "\n",
        "\n",
        "    # Save history\n",
        "    with open(output_dir + filename + '_hist.json', 'w') as f:\n",
        "        json.dump(hist.history, f)\n",
        "    history = hist.history\n",
        "        \n",
        "    # Save the weights\n",
        "    autoencoder.save_weights(output_dir + filename + '_weights.h5')\n",
        "else:\n",
        "    # Load weights\n",
        "    autoencoder.load_weights(f'{output_dir}{filename}_weights.h5')\n",
        "\n",
        "    # Load history\n",
        "    with open(f'{output_dir}{filename}_hist.json', 'r') as f:\n",
        "        history = json.load(f)\n",
        "\n",
        "autoencoder.save(output_dir + filename + '.h5', include_optimizer=False)\n",
        "        \n",
        "# Plot training history \n",
        "plt.plot(history['loss'], linewidth=3, label='train')\n",
        "plt.plot(history['val_loss'], linewidth=3, label='valid')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.ylim(1e-2, 0.1)\n",
        "plt.ylim(1e-5, 1e-3) #mse\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ur-JWTe4WI0y"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fYGvPDyWLQz"
      },
      "source": [
        "x_test = x_original[2:]\n",
        "x_noise_test = x_noise[2:]\n",
        "x_test_noisy = x_train_noisy[2:]\n",
        "decoded_imgs = autoencoder.predict(x_test_noisy)\n",
        "\n",
        "# revert scale and offset\n",
        "x_noise_test -= scale * offset\n",
        "x_noise_test /= scale\n",
        "x_test_noisy -= scale * offset\n",
        "x_test_noisy /= scale\n",
        "decoded_imgs -= scale * offset\n",
        "decoded_imgs /= scale\n",
        "x_subtracted = x_noise_test - decoded_imgs\n",
        "\n",
        "# How many waveforms to be displayed\n",
        "n = 1\n",
        "plt.figure(figsize=(20, 6))\n",
        "for i in range(n):\n",
        "    plt.plot(x_test[i], label=\"original\")\n",
        "    plt.plot(x_test_noisy[i], label=\"noisy\")\n",
        "    plt.plot(decoded_imgs[i], label=\"decoded noise\")\n",
        "    #plt.plot(x_noise_test[i], label=\"noise\")\n",
        "    #plt.plot(x_subtracted[i], label=\"subtracted\")\n",
        "    plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H42-gn6xUocT"
      },
      "source": [
        "# Send this plot to comet\n",
        "experiment.log_figure(figure=plt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXYB7Ai4VYhB"
      },
      "source": [
        "if plot_data:\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}